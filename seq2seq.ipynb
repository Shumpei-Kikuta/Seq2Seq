{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kncGiksJA6OX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from math import ceil\n",
    "import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "78jiih2gsN1c"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = sequence.load_data()\n",
    "# print(x_train[: 10, :])\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "# print(x_train[:10, :])\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = torch.tensor(x_train.copy()).to(device)\n",
    "x_test = torch.tensor(x_test.copy()).to(device)\n",
    "y_train = torch.tensor(y_train.copy()).to(device)\n",
    "y_test = torch.tensor(y_test.copy()).to(device)\n",
    "\n",
    "# hyper-parameter\n",
    "MAX_INPUT = 7\n",
    "MAX_OUTPUT = 5\n",
    "HIDDEN_DIM = 32\n",
    "EMB_DIM = 32\n",
    "vocab_size = len(char_to_id)\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 1000\n",
    "batch_num = ceil(len(x_train) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o-jfsKGE8Sv3"
   },
   "outputs": [],
   "source": [
    "def train_loader(data, target, batch_size=100):\n",
    "    input_batchs = []\n",
    "    output_batchs = []\n",
    "#     data, target = shuffle(data, target)\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        if i == batch_num - 1:\n",
    "            each_input_batchs =  data[i * batch_size:]\n",
    "            each_output_batchs = target[i * batch_size:]\n",
    "        else:\n",
    "            each_input_batchs =  data[i * batch_size: (i + 1) * batch_size]\n",
    "            each_output_batchs = target[i * batch_size:(i + 1) * batch_size]\n",
    "        input_batchs.append(each_input_batchs)\n",
    "        output_batchs.append(each_output_batchs)\n",
    "    return input_batchs, output_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "onLDt0Pc8Sv5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iRK7dhDyvlsM"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim) # padding_idx=5 #num_embeddings: inputの系列の長さ\n",
    "    # 単語の分散表現の初期化\n",
    "#     self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight)) #今回はいらない    \n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, dropout=0.5)\n",
    "\n",
    "    def forward(self, indices, batch_size=100):\n",
    "        # indices = tensor([batch_size, MAX_INPUT(7)])\n",
    "        embedding = self.embedding(indices)\n",
    "        if embedding.dim() == 2: #　バッチサイズが1の時3次元にしている\n",
    "            embedding = torch.unsqueeze(embedding, 1)\n",
    "        _, state = self.gru(embedding, torch.zeros(1, batch_size, self.hidden_dim).to(device)) #最初の入力は0ベクトル\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WiA_8ntc8Ypp"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, dropout=0.5)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, indices, init_hidden):\n",
    "        embedding = self.embedding(indices)\n",
    "        if embedding.dim() == 2: #バッチサイズが1の時3次元に変換\n",
    "            embedding = torch.unsqueeze(embedding, 1)\n",
    "        output, state = self.gru(embedding, init_hidden) #最初の入力は0ベクトル\n",
    "        output = self.linear(output)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SLh7X4m_8SwA"
   },
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        batch_size = encoder_input.size(0)\n",
    "        encoder_hidden = self.encoder(encoder_input, batch_size)\n",
    "        \n",
    "        source = decoder_input[:, :-1]\n",
    "        target = decoder_input[:, 1:]\n",
    "        \n",
    "        # 1文字ずつ入力し、outputのcross entropyを計算する\n",
    "        loss = 0\n",
    "        batch_size = encoder_hidden.size(1)\n",
    "        target_length = target.size(1)\n",
    "        source_length = source.size(1)\n",
    "        decoder_output = np.zeros((batch_size, target_length))\n",
    "        for i in range(source_length):\n",
    "            decoder_result, _ = self.decoder(source[:, i], encoder_hidden)\n",
    "            decoder_result = torch.squeeze(decoder_result)\n",
    "            decoder_output[:, i] = np.argmax(decoder_result.cpu().detach().numpy(), axis=1)\n",
    "            loss += self.criterion(decoder_result, target[:, i])\n",
    "        return loss, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o70D9sx98SwD"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=LEARNING_RATE)\n",
    "        self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # 重みの初期化\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        self.train_loss, self.train_output =  self.model.forward(X, Y)\n",
    "\n",
    "        # backward\n",
    "        self.train_loss.backward()\n",
    "        self.train_loss = self.train_loss.item()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        return self.train_output, self.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "cqCcpGah8SwF",
    "outputId": "40488cae-958a-4bf4-e337-c90974db579c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Seq2seq()\n",
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XTzuXQ_28SwK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6QN6GqW8SwO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cQZC0v4s8SwT"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(pred_Y, Y):\n",
    "    \"\"\"pred_Yと、Yが列方向に一致しているか\"\"\"\n",
    "    same_num = np.equal(pred_Y, Y).all(axis=1).sum()\n",
    "    same_rate = same_num / len(Y)\n",
    "    return same_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "kVoQllgT8SwV",
    "outputId": "c6454ca9-9edb-4be1-cbcd-3db5dbee9d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pred_Y = np.array([[1, 2, 3], [2, 3, 1]])\n",
    "Y = np.array([[1, 2, 3], [2, 2, 2]])\n",
    "print(calc_accuracy(pred_Y, Y) == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "0TKEeylv8SwZ",
    "outputId": "8f7df165-dcfb-4a0d-d0a1-dcdf3e86c313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pred_Y = np.array([[1, 2, 3], [2, 3, 1], [3, 3, 4], [1, 1, 1]])\n",
    "Y = np.array([[1, 2, 3], [2, 2, 2], [3, 3, 4], [1, 1, 1]])\n",
    "print(calc_accuracy(pred_Y, Y) == 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "XD0Yvwc-8Swd",
    "outputId": "25c89bfe-cf96-4ba1-bee2-245a8993e517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n",
      "8.888888888888889e-05 0.0\n",
      "0.017573798497517903 7.994502067565918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a608a45629ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_batchs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_batch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e189feee9df7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training..\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "train_answer = y_train[:, 1:]\n",
    "test_answer = y_test[:, 1:]\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    train_loss = 0\n",
    "    input_batchs, output_batchs = train_loader(x_train, y_train)\n",
    "\n",
    "    for i in range(len(input_batchs)): # mini-batchごとに最適化\n",
    "        input_batch = input_batchs[i].to(device)\n",
    "        output_batch = output_batchs[i].to(device)\n",
    "\n",
    "        train_output, train_loss = trainer.fit(input_batch, output_batch)\n",
    "        train_batch_outputs = np.concatenate([train_batch_outputs, train_output], axis=0) if i != 0 else train_output\n",
    "\n",
    "    train_loss /= batch_num\n",
    "\n",
    "    train_accuracy = calc_accuracy(train_batch_outputs, train_answer.cpu().numpy())\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    test_loss, test_outputs = model.forward(x_test, y_test)\n",
    "        \n",
    "    test_loss = test_loss.item()\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    test_accuracy = calc_accuracy(test_outputs, test_answer.cpu().numpy())\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(train_accuracy, test_accuracy)\n",
    "    print(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9v5x14qu8Swg"
   },
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WXZpB5JA8Swj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
