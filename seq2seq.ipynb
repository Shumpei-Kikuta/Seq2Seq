{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kncGiksJA6OX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from math import ceil\n",
    "import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "78jiih2gsN1c"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = sequence.load_data()\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = torch.tensor(x_train)\n",
    "x_test = torch.tensor(x_test)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# hyper-parameter\n",
    "MAX_INPUT = 7\n",
    "MAX_OUTPUT = 5\n",
    "HIDDEN_DIM = 32\n",
    "EMB_DIM = 32\n",
    "vocab_size = len(char_to_id)\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 100\n",
    "batch_num = ceil(len(x_train) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_loader(data, target, batch_size=100):\n",
    "    input_batchs = []\n",
    "    output_batchs = []\n",
    "#     data, target = shuffle(data, target)\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        if i == batch_num - 1:\n",
    "            each_input_batchs =  data[i * batch_size:]\n",
    "            each_output_batchs = target[i * batch_size:]\n",
    "        else:\n",
    "            each_input_batchs =  data[i * batch_size: (i + 1) * batch_size]\n",
    "            each_output_batchs = target[i * batch_size:(i + 1) * batch_size]\n",
    "        input_batchs.append(each_input_batchs)\n",
    "        output_batchs.append(each_output_batchs)\n",
    "    return input_batchs, output_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "iRK7dhDyvlsM"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim) # padding_idx=5 #num_embeddings: inputの系列の長さ\n",
    "    # 単語の分散表現の初期化\n",
    "#     self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight)) #今回はいらない    \n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, indices, batch_size=100):\n",
    "        # indices = tensor([batch_size, MAX_INPUT(7)])\n",
    "        embedding = self.embedding(indices)\n",
    "        if embedding.dim() == 2: #　バッチサイズが1の時3次元にしている\n",
    "            embedding = torch.unsqueeze(embedding, 1)\n",
    "        _, state = self.gru(embedding, torch.zeros(1, batch_size, self.hidden_dim).to(device)) #最初の入力は0ベクトル\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "WiA_8ntc8Ypp"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, indices, init_hidden):\n",
    "        embedding = self.embedding(indices)\n",
    "        if embedding.dim() == 2: #バッチサイズが1の時3次元に変換\n",
    "            embedding = torch.unsqueeze(embedding, 1)\n",
    "        output, state = self.gru(embedding, init_hidden) #最初の入力は0ベクトル\n",
    "        output = self.linear(output)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        batch_size = encoder_input.size(0)\n",
    "        encoder_hidden = self.encoder(encoder_input, batch_size)\n",
    "        \n",
    "        source = decoder_input[:, :-1]\n",
    "        target = decoder_input[:, 1:]\n",
    "        \n",
    "        # 1文字ずつ入力し、outputのcross entropyを計算する\n",
    "        loss = 0\n",
    "        batch_size = encoder_hidden.size(1)\n",
    "        target_length = target.size(1)\n",
    "        source_length = source.size(1)\n",
    "        decoder_output = np.zeros((batch_size, target_length))\n",
    "        for i in range(source_length):\n",
    "            decoder_result, _ = self.decoder(source[:, i], encoder_hidden)\n",
    "            decoder_result = torch.squeeze(decoder_result)\n",
    "            decoder_output[:, i] = np.argmax(decoder_result.detach().numpy(), axis=1)\n",
    "            loss += self.criterion(decoder_result, target[:, i])\n",
    "        return loss, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=LEARNING_RATE)\n",
    "        self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # 重みの初期化\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        self.train_loss, self.train_output =  self.model.forward(X, Y)\n",
    "\n",
    "        # backward\n",
    "        self.train_loss.backward()\n",
    "        self.train_loss = self.train_loss.item()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.train_output\n",
    "    \n",
    "    def get_loss(self):\n",
    "        return self.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Seq2seq()\n",
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(pred_Y, Y).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(pred_Y, Y):\n",
    "    \"\"\"pred_Yと、Yが列方向に一致しているか\"\"\"\n",
    "    same_num = np.equal(pred_Y, Y).all(axis=1).sum()\n",
    "    same_rate = same_num / len(Y)\n",
    "    return same_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pred_Y = np.array([[1, 2, 3], [2, 3, 1]])\n",
    "Y = np.array([[1, 2, 3], [2, 2, 2]])\n",
    "print(calc_accuracy(pred_Y, Y) == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pred_Y = np.array([[1, 2, 3], [2, 3, 1], [3, 3, 4], [1, 1, 1]])\n",
    "Y = np.array([[1, 2, 3], [2, 2, 2], [3, 3, 4], [1, 1, 1]])\n",
    "print(calc_accuracy(pred_Y, Y) == 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n",
      "0.006977777777777778 0.0062\n",
      "5.394304948382907 5.454184532165527\n",
      "0.007244444444444445 0.0068\n",
      "5.370049650404188 5.432393550872803\n",
      "0.007311111111111111 0.0076\n",
      "5.346923588646783 5.411534786224365\n",
      "0.0077555555555555555 0.008\n",
      "5.324688367843628 5.389551639556885\n",
      "0.0077555555555555555 0.0084\n",
      "5.302518379423353 5.367369651794434\n"
     ]
    }
   ],
   "source": [
    "print(\"Training..\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "train_answer = y_train[:, 1:]\n",
    "test_answer = y_test[:, 1:]\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    train_loss = 0\n",
    "    input_batchs, output_batchs = train_loader(x_train, y_train)\n",
    "\n",
    "#     train_batch_outputs = \n",
    "    for i in range(len(input_batchs)): # mini-batchごとに最適化\n",
    "        input_batch = input_batchs[i].to(device)\n",
    "        output_batch = output_batchs[i].to(device)\n",
    "\n",
    "        trainer.fit(input_batch, output_batch)\n",
    "        train_output = trainer.predict()\n",
    "        train_batch_outputs = np.concatenate([train_batch_outputs, train_output], axis=0) if i != 0 else train_output\n",
    "        train_loss += trainer.get_loss()\n",
    "\n",
    "    train_loss /= batch_num\n",
    "\n",
    "    train_accuracy = calc_accuracy(train_batch_outputs, train_answer.numpy())\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    test_loss, test_outputs = model.forward(x_test, y_test)\n",
    "        \n",
    "    test_loss = test_loss.item()\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    test_accuracy = calc_accuracy(test_outputs, test_answer.numpy())\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(train_accuracy, test_accuracy)\n",
    "    print(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracyの計算\n",
    "# loaderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
