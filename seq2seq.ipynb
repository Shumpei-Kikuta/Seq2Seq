{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kncGiksJA6OX",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.utils import shuffle\n",
        "from math import ceil\n",
        "import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "78jiih2gsN1c",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = sequence.load_data()\n",
        "# print(x_train[: 10, :])\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "# print(x_train[:10, :])\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x_train = torch.tensor(x_train.copy()).to(device)\n",
        "x_test = torch.tensor(x_test.copy()).to(device)\n",
        "y_train = torch.tensor(y_train.copy()).to(device)\n",
        "y_test = torch.tensor(y_test.copy()).to(device)\n",
        "\n",
        "# hyper-parameter\n",
        "MAX_INPUT = 7\n",
        "MAX_OUTPUT = 5\n",
        "HIDDEN_DIM = 32\n",
        "EMB_DIM = 32\n",
        "vocab_size = len(char_to_id)\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 100\n",
        "EPOCH = 1000\n",
        "batch_num = ceil(len(x_train) // BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-jfsKGE8Sv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loader(data, target, batch_size=100):\n",
        "    input_batchs = []\n",
        "    output_batchs = []\n",
        "#     data, target = shuffle(data, target)\n",
        "    \n",
        "    for i in range(batch_num):\n",
        "        if i == batch_num - 1:\n",
        "            each_input_batchs =  data[i * batch_size:]\n",
        "            each_output_batchs = target[i * batch_size:]\n",
        "        else:\n",
        "            each_input_batchs =  data[i * batch_size: (i + 1) * batch_size]\n",
        "            each_output_batchs = target[i * batch_size:(i + 1) * batch_size]\n",
        "        input_batchs.append(each_input_batchs)\n",
        "        output_batchs.append(each_output_batchs)\n",
        "    return input_batchs, output_batchs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onLDt0Pc8Sv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iRK7dhDyvlsM",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim) # padding_idx=5 #num_embeddings: inputの系列の長さ\n",
        "    # 単語の分散表現の初期化\n",
        "#     self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight)) #今回はいらない    \n",
        "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, dropout=0.5)\n",
        "\n",
        "    def forward(self, indices, batch_size=100):\n",
        "        # indices = tensor([batch_size, MAX_INPUT(7)])\n",
        "        embedding = self.embedding(indices)\n",
        "        if embedding.dim() == 2: #　バッチサイズが1の時3次元にしている\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        _, state = self.gru(embedding, torch.zeros(1, batch_size, self.hidden_dim).to(device)) #最初の入力は0ベクトル\n",
        "        return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiA_8ntc8Ypp",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim)\n",
        "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, dropout=0.5)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, indices, init_hidden):\n",
        "        embedding = self.embedding(indices)\n",
        "        if embedding.dim() == 2: #バッチサイズが1の時3次元に変換\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        output, state = self.gru(embedding, init_hidden) #最初の入力は0ベクトル\n",
        "        output = self.linear(output)\n",
        "        return output, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLh7X4m_8SwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq:\n",
        "    def __init__(self):\n",
        "        self.encoder = Encoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
        "        self.decoder = Decoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size = encoder_input.size(0)\n",
        "        encoder_hidden = self.encoder(encoder_input, batch_size)\n",
        "        \n",
        "        source = decoder_input[:, :-1]\n",
        "        target = decoder_input[:, 1:]\n",
        "        \n",
        "        # 1文字ずつ入力し、outputのcross entropyを計算する\n",
        "        loss = 0\n",
        "        batch_size = encoder_hidden.size(1)\n",
        "        target_length = target.size(1)\n",
        "        source_length = source.size(1)\n",
        "        decoder_output = np.zeros((batch_size, target_length))\n",
        "        for i in range(source_length):\n",
        "            decoder_result, _ = self.decoder(source[:, i], encoder_hidden)\n",
        "            decoder_result = torch.squeeze(decoder_result)\n",
        "            decoder_output[:, i] = np.argmax(decoder_result.cpu().detach().numpy(), axis=1)\n",
        "            loss += self.criterion(decoder_result, target[:, i])\n",
        "        return loss, decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o70D9sx98SwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=LEARNING_RATE)\n",
        "        self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n",
        "        \n",
        "    def fit(self, X, Y):\n",
        "        # 重みの初期化\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "        \n",
        "        self.train_loss, self.train_output =  self.model.forward(X, Y)\n",
        "\n",
        "        # backward\n",
        "        self.train_loss.backward()\n",
        "        self.train_loss = self.train_loss.item()\n",
        "\n",
        "        self.encoder_optimizer.step()\n",
        "        self.decoder_optimizer.step()\n",
        "        \n",
        "    def predict(self):\n",
        "        return self.train_output\n",
        "    \n",
        "    def get_loss(self):\n",
        "        return self.train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqCcpGah8SwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "40488cae-958a-4bf4-e337-c90974db579c"
      },
      "source": [
        "model = Seq2seq()\n",
        "trainer = Trainer(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTzuXQ_28SwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6QN6GqW8SwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQZC0v4s8SwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(pred_Y, Y):\n",
        "    \"\"\"pred_Yと、Yが列方向に一致しているか\"\"\"\n",
        "    same_num = np.equal(pred_Y, Y).all(axis=1).sum()\n",
        "    same_rate = same_num / len(Y)\n",
        "    return same_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVoQllgT8SwV",
        "colab_type": "code",
        "outputId": "c6454ca9-9edb-4be1-cbcd-3db5dbee9d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_Y = np.array([[1, 2, 3], [2, 3, 1]])\n",
        "Y = np.array([[1, 2, 3], [2, 2, 2]])\n",
        "print(calc_accuracy(pred_Y, Y) == 0.5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TKEeylv8SwZ",
        "colab_type": "code",
        "outputId": "8f7df165-dcfb-4a0d-d0a1-dcdf3e86c313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_Y = np.array([[1, 2, 3], [2, 3, 1], [3, 3, 4], [1, 1, 1]])\n",
        "Y = np.array([[1, 2, 3], [2, 2, 2], [3, 3, 4], [1, 1, 1]])\n",
        "print(calc_accuracy(pred_Y, Y) == 0.75)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD0Yvwc-8Swd",
        "colab_type": "code",
        "outputId": "25c89bfe-cf96-4ba1-bee2-245a8993e517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Training..\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "train_answer = y_train[:, 1:]\n",
        "test_answer = y_test[:, 1:]\n",
        "\n",
        "for e in range(EPOCH):\n",
        "    train_loss = 0\n",
        "    input_batchs, output_batchs = train_loader(x_train, y_train)\n",
        "\n",
        "#     train_batch_outputs = \n",
        "    for i in range(len(input_batchs)): # mini-batchごとに最適化\n",
        "        input_batch = input_batchs[i].to(device)\n",
        "        output_batch = output_batchs[i].to(device)\n",
        "\n",
        "        trainer.fit(input_batch, output_batch)\n",
        "        train_output = trainer.predict()\n",
        "        train_batch_outputs = np.concatenate([train_batch_outputs, train_output], axis=0) if i != 0 else train_output\n",
        "        train_loss += trainer.get_loss()\n",
        "\n",
        "    train_loss /= batch_num\n",
        "\n",
        "    train_accuracy = calc_accuracy(train_batch_outputs, train_answer.cpu().numpy())\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    test_loss, test_outputs = model.forward(x_test, y_test)\n",
        "        \n",
        "    test_loss = test_loss.item()\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    test_accuracy = calc_accuracy(test_outputs, test_answer.cpu().numpy())\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    \n",
        "    print(train_accuracy, test_accuracy)\n",
        "    print(train_loss, test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training..\n",
            "8.888888888888889e-05 0.0002\n",
            "8.586916179656983 7.959208011627197\n",
            "0.0008444444444444444 0.0014\n",
            "7.292317153082953 6.847160339355469\n",
            "0.001488888888888889 0.0016\n",
            "6.630344058142768 6.440935134887695\n",
            "0.0018 0.002\n",
            "6.318574911753337 6.196824550628662\n",
            "0.0029111111111111113 0.0032\n",
            "6.097347436481052 6.00013542175293\n",
            "0.003955555555555556 0.0034\n",
            "5.905274489720663 5.825521469116211\n",
            "0.004977777777777778 0.0046\n",
            "5.727414986292521 5.664422988891602\n",
            "0.005866666666666667 0.0048\n",
            "5.568377659055922 5.515938758850098\n",
            "0.006733333333333333 0.005\n",
            "5.430255439546373 5.391399383544922\n",
            "0.007222222222222222 0.006\n",
            "5.3153769164615206 5.293417930603027\n",
            "0.00811111111111111 0.0068\n",
            "5.2213721105787485 5.212296485900879\n",
            "0.0094 0.008\n",
            "5.141748424106174 5.139065265655518\n",
            "0.010866666666666667 0.0088\n",
            "5.0725747150844995 5.0751118659973145\n",
            "0.012244444444444445 0.0098\n",
            "5.01244526969062 5.019503116607666\n",
            "0.012533333333333334 0.0104\n",
            "4.959148616790771 4.970394134521484\n",
            "0.0138 0.0104\n",
            "4.9108379936218265 4.926636695861816\n",
            "0.0148 0.011\n",
            "4.866187852223714 4.8874311447143555\n",
            "0.015955555555555556 0.0124\n",
            "4.824762913386027 4.850967884063721\n",
            "0.01682222222222222 0.0146\n",
            "4.786476315392388 4.816974639892578\n",
            "0.018444444444444444 0.0164\n",
            "4.750861902236938 4.7852301597595215\n",
            "0.019844444444444446 0.0184\n",
            "4.717681928210788 4.755444526672363\n",
            "0.020866666666666665 0.0198\n",
            "4.686816425323486 4.727450370788574\n",
            "0.022133333333333335 0.0204\n",
            "4.658049675623576 4.701239585876465\n",
            "0.023755555555555554 0.022\n",
            "4.631102768580119 4.676424980163574\n",
            "0.0248 0.023\n",
            "4.605622964435153 4.652846336364746\n",
            "0.025755555555555556 0.0228\n",
            "4.581440375645955 4.630326271057129\n",
            "0.026733333333333335 0.0242\n",
            "4.558859550688002 4.609104156494141\n",
            "0.027266666666666668 0.0252\n",
            "4.537561847898695 4.588809013366699\n",
            "0.027866666666666668 0.0262\n",
            "4.517364734013875 4.5692338943481445\n",
            "0.029222222222222222 0.0266\n",
            "4.498075407875909 4.550375938415527\n",
            "0.0304 0.0276\n",
            "4.479524184332954 4.532294750213623\n",
            "0.031088888888888888 0.0284\n",
            "4.461621420118544 4.5150346755981445\n",
            "0.032377777777777776 0.0278\n",
            "4.444296779632569 4.498592376708984\n",
            "0.033155555555555556 0.0278\n",
            "4.427472809685601 4.483006954193115\n",
            "0.03395555555555556 0.0284\n",
            "4.4111124939388695 4.468311786651611\n",
            "0.03464444444444444 0.0294\n",
            "4.395186873012119 4.454409599304199\n",
            "0.03582222222222222 0.0296\n",
            "4.3796809758080375 4.441204071044922\n",
            "0.03668888888888889 0.0294\n",
            "4.364622236887614 4.428645133972168\n",
            "0.03757777777777778 0.032\n",
            "4.350007914437188 4.416611671447754\n",
            "0.038533333333333336 0.0324\n",
            "4.335798361036512 4.404955863952637\n",
            "0.03913333333333333 0.0324\n",
            "4.3219022713767155 4.39380407333374\n",
            "0.04031111111111111 0.0322\n",
            "4.308286264207628 4.382707595825195\n",
            "0.040688888888888886 0.0322\n",
            "4.294966309865316 4.371397018432617\n",
            "0.04182222222222222 0.0326\n",
            "4.282026823361715 4.360211372375488\n",
            "0.04308888888888889 0.0344\n",
            "4.269478874206543 4.349366664886475\n",
            "0.04426666666666667 0.036\n",
            "4.2572680902481075 4.3389692306518555\n",
            "0.045422222222222225 0.0364\n",
            "4.2453576713138155 4.329071521759033\n",
            "0.04646666666666667 0.0376\n",
            "4.233732696639167 4.319630146026611\n",
            "0.04751111111111111 0.039\n",
            "4.222387954923842 4.310508728027344\n",
            "0.04833333333333333 0.0394\n",
            "4.211320589913262 4.30172872543335\n",
            "0.04928888888888889 0.0408\n",
            "4.200547762446933 4.2933807373046875\n",
            "0.050444444444444445 0.0408\n",
            "4.1900478394826255 4.285268306732178\n",
            "0.051644444444444444 0.0412\n",
            "4.179752848943075 4.277239799499512\n",
            "0.052444444444444446 0.0416\n",
            "4.169645717408922 4.269266605377197\n",
            "0.05311111111111111 0.042\n",
            "4.1597468328475955 4.261137962341309\n",
            "0.05411111111111111 0.0422\n",
            "4.150036019749112 4.252587795257568\n",
            "0.05477777777777778 0.0422\n",
            "4.1404603841569685 4.243683338165283\n",
            "0.05533333333333333 0.043\n",
            "4.13105325963762 4.234601974487305\n",
            "0.05631111111111111 0.0434\n",
            "4.121835734579299 4.225371360778809\n",
            "0.05677777777777778 0.0436\n",
            "4.112756665017869 4.215995788574219\n",
            "0.05784444444444444 0.0454\n",
            "4.103769096798367 4.206578731536865\n",
            "0.05842222222222222 0.047\n",
            "4.0948422691557145 4.197271347045898\n",
            "0.059533333333333334 0.0474\n",
            "4.085932592815823 4.187947750091553\n",
            "0.060333333333333336 0.0482\n",
            "4.077133702172174 4.17852783203125\n",
            "0.061066666666666665 0.0498\n",
            "4.068418408499824 4.169277191162109\n",
            "0.06177777777777778 0.0488\n",
            "4.059705549875895 4.159915924072266\n",
            "0.06268888888888889 0.0486\n",
            "4.051152387195163 4.15037727355957\n",
            "0.06351111111111112 0.048\n",
            "4.042810599539015 4.140846252441406\n",
            "0.06433333333333334 0.0498\n",
            "4.034644606378343 4.131321430206299\n",
            "0.06537777777777778 0.0504\n",
            "4.026619854503208 4.12177038192749\n",
            "0.06591111111111111 0.0526\n",
            "4.018704988161723 4.112165451049805\n",
            "0.06637777777777777 0.0522\n",
            "4.010874396430122 4.1025261878967285\n",
            "0.06697777777777777 0.0534\n",
            "4.0031150759591 4.092942237854004\n",
            "0.06755555555555555 0.0552\n",
            "3.9954332669576007 4.083513259887695\n",
            "0.06846666666666666 0.055\n",
            "3.987848448223538 4.074308395385742\n",
            "0.06924444444444444 0.056\n",
            "3.980374000867208 4.065358638763428\n",
            "0.06995555555555556 0.0572\n",
            "3.9730045149061417 4.05664587020874\n",
            "0.07062222222222223 0.0598\n",
            "3.965726866722107 4.048138618469238\n",
            "0.07084444444444445 0.0604\n",
            "3.9585338672002157 4.039894104003906\n",
            "0.0714888888888889 0.0612\n",
            "3.9514219088024563 4.031912803649902\n",
            "0.0722 0.0628\n",
            "3.9443832074271308 4.024065017700195\n",
            "0.07297777777777778 0.0646\n",
            "3.93739419248369 4.016342639923096\n",
            "0.07355555555555555 0.0642\n",
            "3.930414204597473 4.008820533752441\n",
            "0.07417777777777777 0.0654\n",
            "3.923436707390679 4.001511096954346\n",
            "0.07482222222222222 0.0654\n",
            "3.91650428560045 3.994401454925537\n",
            "0.07535555555555555 0.066\n",
            "3.90963643444909 3.987459182739258\n",
            "0.07584444444444445 0.0668\n",
            "3.902817408243815 3.9806017875671387\n",
            "0.07637777777777778 0.0676\n",
            "3.8959905582004124 3.9735989570617676\n",
            "0.07717777777777778 0.0696\n",
            "3.889141505029466 3.966420888900757\n",
            "0.07786666666666667 0.0696\n",
            "3.882412044737074 3.9597158432006836\n",
            "0.07891111111111111 0.0698\n",
            "3.8758789825439455 3.9534144401550293\n",
            "0.0801111111111111 0.071\n",
            "3.869461645020379 3.9473586082458496\n",
            "0.08037777777777778 0.0716\n",
            "3.863115250269572 3.941490888595581\n",
            "0.08071111111111111 0.0726\n",
            "3.8568488778008354 3.9357986450195312\n",
            "0.08162222222222222 0.0722\n",
            "3.8506739150153266 3.9302539825439453\n",
            "0.08228888888888888 0.0728\n",
            "3.8445808214611477 3.924802303314209\n",
            "0.08324444444444444 0.0734\n",
            "3.8385593218273586 3.919394016265869\n",
            "0.08448888888888889 0.0724\n",
            "3.8325808631049263 3.9140071868896484\n",
            "0.08553333333333334 0.073\n",
            "3.826606872346666 3.908534288406372\n",
            "0.08624444444444444 0.072\n",
            "3.820561801592509 3.9028971195220947\n",
            "0.08717777777777778 0.0724\n",
            "3.814410467677646 3.89774751663208\n",
            "0.0876 0.0724\n",
            "3.808458841641744 3.892152786254883\n",
            "0.08857777777777778 0.0738\n",
            "3.8024912791781955 3.8870182037353516\n",
            "0.08955555555555555 0.075\n",
            "3.7967010683483546 3.881826162338257\n",
            "0.0898888888888889 0.0762\n",
            "3.790924426714579 3.8766117095947266\n",
            "0.09048888888888888 0.0766\n",
            "3.7851961019304063 3.8714358806610107\n",
            "0.09073333333333333 0.0784\n",
            "3.7795689254336886 3.8665828704833984\n",
            "0.09122222222222222 0.0804\n",
            "3.7741045983632406 3.8618359565734863\n",
            "0.09217777777777778 0.0808\n",
            "3.768692756758796 3.8571176528930664\n",
            "0.09257777777777777 0.0816\n",
            "3.7633295366499158 3.8523166179656982\n",
            "0.09313333333333333 0.0824\n",
            "3.758011515935262 3.8475241661071777\n",
            "0.09364444444444445 0.083\n",
            "3.7527403089735243 3.8428711891174316\n",
            "0.09448888888888889 0.0828\n",
            "3.7475246784422134 3.8384313583374023\n",
            "0.0954 0.0836\n",
            "3.7423427051968043 3.8335180282592773\n",
            "0.09553333333333333 0.0836\n",
            "3.7372769673665363 3.8289785385131836\n",
            "0.09573333333333334 0.0838\n",
            "3.7322518571217853 3.8246092796325684\n",
            "0.0964 0.0846\n",
            "3.727223245302836 3.820244789123535\n",
            "0.0971111111111111 0.0856\n",
            "3.7222240935431588 3.8159115314483643\n",
            "0.09846666666666666 0.0862\n",
            "3.7172861607869465 3.811603546142578\n",
            "0.09924444444444444 0.0856\n",
            "3.712412783834669 3.807302951812744\n",
            "0.09948888888888889 0.0866\n",
            "3.7075886816448635 3.8030037879943848\n",
            "0.09975555555555556 0.088\n",
            "3.702802220980326 3.798725128173828\n",
            "0.1006 0.0886\n",
            "3.698049063152737 3.7944867610931396\n",
            "0.10144444444444445 0.088\n",
            "3.6933144654168024 3.7903006076812744\n",
            "0.10175555555555556 0.0888\n",
            "3.6885733291837903 3.7861175537109375\n",
            "0.1024888888888889 0.0884\n",
            "3.683817540274726 3.781968116760254\n",
            "0.10284444444444445 0.088\n",
            "3.6790793736775718 3.77805495262146\n",
            "0.10286666666666666 0.0888\n",
            "3.674401786592272 3.774322748184204\n",
            "0.10326666666666667 0.0892\n",
            "3.669816748301188 3.7705554962158203\n",
            "0.10351111111111111 0.0886\n",
            "3.6652867460250853 3.766749858856201\n",
            "0.10342222222222222 0.0882\n",
            "3.6607166030671863 3.762974977493286\n",
            "0.1038 0.0868\n",
            "3.6559956147935657 3.758711576461792\n",
            "0.10402222222222222 0.0874\n",
            "3.651172574361165 3.7542221546173096\n",
            "0.105 0.088\n",
            "3.6462848642137318 3.749577522277832\n",
            "0.10584444444444445 0.0898\n",
            "3.6413750722673206 3.744767189025879\n",
            "0.10611111111111111 0.0904\n",
            "3.6365113131205242 3.7400269508361816\n",
            "0.10675555555555556 0.0916\n",
            "3.631712359322442 3.735567569732666\n",
            "0.10728888888888889 0.0924\n",
            "3.626979773839315 3.73134708404541\n",
            "0.10768888888888889 0.0924\n",
            "3.622329157193502 3.727252244949341\n",
            "0.10826666666666666 0.0926\n",
            "3.617764506340027 3.7231602668762207\n",
            "0.10873333333333333 0.0926\n",
            "3.6132841528786552 3.7189724445343018\n",
            "0.10937777777777778 0.093\n",
            "3.608888577355279 3.7148139476776123\n",
            "0.10995555555555556 0.0942\n",
            "3.604530456331041 3.710712432861328\n",
            "0.11075555555555555 0.0956\n",
            "3.6001312499576144 3.7062056064605713\n",
            "0.11122222222222222 0.0958\n",
            "3.595690321392483 3.701368808746338\n",
            "0.11193333333333333 0.0964\n",
            "3.591198166741265 3.69581937789917\n",
            "0.11257777777777778 0.0972\n",
            "3.5867558595869276 3.6918113231658936\n",
            "0.113 0.0976\n",
            "3.5827469884024725 3.6873438358306885\n",
            "0.11331111111111111 0.0984\n",
            "3.5788654327392577 3.6825942993164062\n",
            "0.11406666666666666 0.1002\n",
            "3.57484642929501 3.678792953491211\n",
            "0.11404444444444445 0.1006\n",
            "3.57069774945577 3.674971342086792\n",
            "0.1146 0.1008\n",
            "3.5666707875993517 3.671588182449341\n",
            "0.11506666666666666 0.1008\n",
            "3.562533229721917 3.667776584625244\n",
            "0.11564444444444444 0.1016\n",
            "3.5583432478374903 3.6639769077301025\n",
            "0.11626666666666667 0.1022\n",
            "3.554113443692525 3.6581919193267822\n",
            "0.11657777777777778 0.1046\n",
            "3.5498287937376234 3.6561012268066406\n",
            "0.11731111111111112 0.104\n",
            "3.5464593664805095 3.6553163528442383\n",
            "0.11733333333333333 0.1048\n",
            "3.54170130888621 3.649013042449951\n",
            "0.11813333333333334 0.1062\n",
            "3.538742201593187 3.6487529277801514\n",
            "0.11826666666666667 0.107\n",
            "3.534080042309231 3.6443684101104736\n",
            "0.11822222222222223 0.1072\n",
            "3.5311877208285862 3.6403605937957764\n",
            "0.11886666666666666 0.1076\n",
            "3.527448744773865 3.6382617950439453\n",
            "0.11944444444444445 0.1068\n",
            "3.522860154575772 3.634380340576172\n",
            "0.12035555555555555 0.1068\n",
            "3.519899024963379 3.629802703857422\n",
            "0.12064444444444444 0.1072\n",
            "3.5158461083306207 3.627161979675293\n",
            "0.12115555555555556 0.1062\n",
            "3.5124532190958657 3.624149799346924\n",
            "0.12133333333333333 0.1082\n",
            "3.508603860007392 3.621131420135498\n",
            "0.12248888888888888 0.1078\n",
            "3.50436117861006 3.61806583404541\n",
            "0.12291111111111111 0.1076\n",
            "3.500569865438673 3.6145544052124023\n",
            "0.12357777777777777 0.1076\n",
            "3.496654797130161 3.6111156940460205\n",
            "0.12386666666666667 0.1104\n",
            "3.493002569410536 3.608139753341675\n",
            "0.12422222222222222 0.1112\n",
            "3.489753722084893 3.6045188903808594\n",
            "0.12562222222222222 0.1102\n",
            "3.4852283599641587 3.600078821182251\n",
            "0.12595555555555554 0.1118\n",
            "3.4814274517695107 3.596201181411743\n",
            "0.1265111111111111 0.114\n",
            "3.4780502637227375 3.594648599624634\n",
            "0.1256 0.1134\n",
            "3.4763832828733654 3.5903067588806152\n",
            "0.12655555555555556 0.1132\n",
            "3.4714759333928424 3.588646173477173\n",
            "0.12684444444444445 0.113\n",
            "3.4687717702653673 3.5870981216430664\n",
            "0.12724444444444444 0.1124\n",
            "3.46531872537401 3.581531286239624\n",
            "0.12773333333333334 0.1142\n",
            "3.461986893547906 3.5820887088775635\n",
            "0.12853333333333333 0.1164\n",
            "3.4585682588153417 3.5734915733337402\n",
            "0.12877777777777777 0.116\n",
            "3.454803548389011 3.5717666149139404\n",
            "0.12893333333333334 0.1154\n",
            "3.4521384032567344 3.5682201385498047\n",
            "0.12957777777777776 0.1156\n",
            "3.4487661515341865 3.565558671951294\n",
            "0.13002222222222223 0.1172\n",
            "3.4453062396579317 3.5621163845062256\n",
            "0.13106666666666666 0.1176\n",
            "3.44238430235121 3.5546369552612305\n",
            "0.13155555555555556 0.1166\n",
            "3.438248243331909 3.5531015396118164\n",
            "0.13186666666666666 0.1192\n",
            "3.4350806951522825 3.5524051189422607\n",
            "0.13206666666666667 0.1208\n",
            "3.432514739566379 3.5491793155670166\n",
            "0.1323111111111111 0.1208\n",
            "3.429433413611518 3.5469462871551514\n",
            "0.13248888888888888 0.1212\n",
            "3.4266100788116454 3.5443875789642334\n",
            "0.13344444444444445 0.1218\n",
            "3.423310621049669 3.541738986968994\n",
            "0.13346666666666668 0.1228\n",
            "3.4202060720655654 3.537614583969116\n",
            "0.13397777777777778 0.1232\n",
            "3.416993384361267 3.532273054122925\n",
            "0.13377777777777777 0.1238\n",
            "3.41386542585161 3.528508424758911\n",
            "0.13391111111111112 0.1244\n",
            "3.4109616872999404 3.5253944396972656\n",
            "0.1341111111111111 0.1242\n",
            "3.408014851146274 3.522266387939453\n",
            "0.13422222222222221 0.1244\n",
            "3.405026774406433 3.5193307399749756\n",
            "0.13473333333333334 0.1248\n",
            "3.40205495039622 3.516345262527466\n",
            "0.13515555555555556 0.1256\n",
            "3.399107265472412 3.5132157802581787\n",
            "0.1356888888888889 0.127\n",
            "3.3962005927827623 3.5100245475769043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v5x14qu8Swg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# loaderの定義"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXZpB5JA8Swj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}