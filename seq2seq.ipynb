{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kncGiksJA6OX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "78jiih2gsN1c"
   },
   "outputs": [],
   "source": [
    "# global 変数\n",
    "(x_train, y_train), (x_test, y_test) = sequence.load_data()\n",
    "x_train = torch.tensor(x_train)\n",
    "x_test = torch.tensor(x_test)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper-parameter\n",
    "MAX_INPUT = 7\n",
    "MAX_OUTPUT = 5\n",
    "HIDDEN_DIM = 32\n",
    "EMB_DIM = 32\n",
    "vocab_size = len(char_to_id)\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "iRK7dhDyvlsM"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim) # padding_idx=5 #num_embeddings: inputの系列の長さ\n",
    "    # 単語の分散表現の初期化\n",
    "#     self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight)) #今回はいらない    \n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, indices, batch_size=100):\n",
    "        # indices = tensor([batch_size, MAX_INPUT(7)])\n",
    "        embedding = self.embedding(indices)\n",
    "        assert(indices.size(1) == 7)\n",
    "        if embedding.dim() == 2:\n",
    "            embedding = torch.unsqueeze(embedding, 1) #3次元にしている\n",
    "        _, state = self.gru(embedding, torch.zeros(1, batch_size, self.hidden_dim).to(device)) #最初の入力は0ベクトル\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WiA_8ntc8Ypp"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, indices, init_hidden):\n",
    "        embedding = self.embedding(indices)\n",
    "        if embedding.dim() == 2:\n",
    "            embedding = torch.unsqueeze(embedding, 1)\n",
    "        output, state = self.gru(embedding, init_hidden) #最初の入力は0ベクトル\n",
    "        output = self.linear(output)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, hidden_dim=HIDDEN_DIM, emb_dim=EMB_DIM).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        batch_size = encoder_input.size(0)\n",
    "        encoder_hidden = self.encoder(encoder_input, batch_size)\n",
    "        \n",
    "        source = decoder_input[:, :-1]\n",
    "        target = decoder_input[:, 1:]\n",
    "        \n",
    "        # 1文字ずつ入力し、outputのcross entropyを計算する\n",
    "        loss = 0\n",
    "        batch_size = encoder_hidden.size(1)\n",
    "        target_length = target.size(1)\n",
    "        source_length = source.size(1)\n",
    "        decoder_output = np.zeros((batch_size, target_length))\n",
    "        for i in range(source_length):\n",
    "            decoder_result, _ = self.decoder(source[:, i], encoder_hidden)\n",
    "            decoder_result = torch.squeeze(decoder_result)\n",
    "            decoder_output[:, i] = np.argmax(decoder_result.detach().numpy(), axis=1)\n",
    "            loss += self.criterion(decoder_result, target[:, i])\n",
    "        return loss, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=LEARNING_RATE)\n",
    "        self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # 重みの初期化\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        self.train_loss, self.train_output =  self.model.forward(X, Y)\n",
    "\n",
    "        # backward\n",
    "        self.train_loss.backward()\n",
    "        self.train_loss = self.train_loss.item()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.train_output\n",
    "    \n",
    "    def get_loss(self):\n",
    "        return self.train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Seq2seq()\n",
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from math import ceil\n",
    "\n",
    "def train_loader(data, target, batch_size=100):\n",
    "    input_batchs = []\n",
    "    output_batchs = []\n",
    "#     data, target = shuffle(data, target)\n",
    "    \n",
    "    batch_num = ceil(len(data) // batch_size)\n",
    "    for i in range(batch_num):\n",
    "        if i == batch_num - 1:\n",
    "            each_input_batchs =  data[i * batch_size:]\n",
    "            each_output_batchs = target[i * batch_size:]\n",
    "        else:\n",
    "            each_input_batchs =  data[i * batch_size: (i + 1) * batch_size]\n",
    "            each_output_batchs = target[i * batch_size:(i + 1) * batch_size]\n",
    "        input_batchs.append(each_input_batchs)\n",
    "        output_batchs.append(each_output_batchs)\n",
    "    return input_batchs, output_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n",
      "10.43000602722168 10.346759796142578\n",
      "10.361082077026367 10.295493125915527\n",
      "10.293654441833496 10.245674133300781\n",
      "10.22774600982666 10.197254180908203\n",
      "10.163341522216797 10.150217056274414\n",
      "10.100420951843262 10.104557991027832\n",
      "10.038954734802246"
     ]
    }
   ],
   "source": [
    "print(\"Training..\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    train_loss = 0\n",
    "    input_batchs, output_batchs = train_loader(x_train, y_train)\n",
    "\n",
    "    for i in range(len(input_batchs)): # mini-batchごとに最適化\n",
    "        input_batch = input_batchs[i].to(device)\n",
    "        output_batch = output_batchs[i].to(device)\n",
    "        \n",
    "#         # 重みの初期化\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         train_loss, train_output =  model.forward(input_batch, output_batch)\n",
    "\n",
    "#         # backward\n",
    "#         train_loss.backward()\n",
    "#         train_loss = train_loss.item()\n",
    "\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "    trainer.fit(input_batch, output_batch)\n",
    "    train_output = trainer.predict()\n",
    "    train_loss = trainer.get_loss()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    test_loss, test_output = model.forward(x_test, y_test)\n",
    "        \n",
    "    test_loss = test_loss.item()\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracyの計算\n",
    "# loaderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
